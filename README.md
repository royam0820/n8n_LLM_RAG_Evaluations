## Ma√Ætriser l‚Äô√âvaluation des LLM et des Workflows dans n8n

Ce d√©p√¥t accompagne mon meetup n8n sur le th√®me **¬´ Ma√Ætriser l‚Äô√âvaluation des LLM et Workflows dans n8n ¬ª**.  
Il regroupe les supports de pr√©sentation, les workflows n8n, les jeux de donn√©es de r√©f√©rence (gold standard) et des scripts Python pour √©valuer vos workflows √† l‚Äô√©chelle (avec l‚ÄôAPI Google Gemini).

---

## Contenu du d√©p√¥t

- **Pr√©sentation principale**
  - `Evaluation workflow n8n.pdf`  
  - Slides utilis√©es pendant le meetup. Elles expliquent les concepts cl√©s :
    - Pourquoi et comment √©valuer un LLM / un workflow n8n  
    - M√©triques d‚Äô√©valuation et cas d‚Äôusage  
    - D√©mo pas‚Äê√†‚Äêpas avec les fichiers de ce d√©p√¥t

- **`EVALUATION/REFERENTIEL` ‚Äì Jeux de donn√©es ¬´ gold standard ¬ª**
  - `1-QA_Evaluations.csv`  
  - `2-QA_LuminaCorp.csv`  
  - **R√¥le**: ensembles de donn√©es de r√©f√©rence servant de v√©rit√© terrain pour comparer les r√©ponses produites par vos workflows n8n / LLM.  
  - **Usage typique**:
    - Colonnes avec *prompt / question*  
    - Colonnes avec *r√©ponse attendue / label de qualit√©*  
    - Utilis√©s par les workflows n8n et le script Python pour calculer des scores d‚Äô√©valuation.

- **`EVALUATION/JSON_QA_Evalutions` ‚Äì Workflows n8n (JSON)**
  - Sous-dossier `Historic_Events/`  
    - `1-Loading Reference Tables.json`  
    - `2-Chat Message to LLM Workflow.json`  
    - `3-Chat Message to LLM Workflow - Pirate Edition.json`  
    - `4-Export QA Evaluations Data to Google Sheet (with Pirate Detection).json`  
    - `5-üìä Evaluation Viz.json`  
  - Sous-dossier `Lumina Corp (RAG)/`
    - `App_RAG Agent - Advanced Citation Evaluation.json`  
  - **R√¥le**: workflows n8n pr√™ts √† l‚Äôemploi pour :
    - Charger les jeux de donn√©es de r√©f√©rence
    - Appeler un LLM (incluant une version ¬´ Pirate Edition ¬ª pour montrer la d√©tection de d√©rives de style)
    - Exporter les r√©sultats d‚Äô√©valuation (par ex. vers Google Sheets)
    - Visualiser les m√©triques d‚Äô√©valuation
    - √âvaluer un agent RAG (LuminaCorp) avec v√©rification avanc√©e des citations
  - **Import dans n8n**:
    1. Ouvrir votre instance n8n.
    2. Cr√©er un nouveau workflow.
    3. Utiliser **Import from file** et choisir le fichier `.json` souhait√©.
    4. Mettre √† jour les credentials (LLM, Google, etc.) selon votre environnement.

- **`EVALUATION/PYTHON` ‚Äì √âvaluation √† l‚Äô√©chelle avec Python & Google Gemini**
  - `rag_evaluator.py`  
  - `RAG_Evaluation_Gemini.ipynb`  
  - **R√¥le**: permettre une √©valuation massive (jusqu‚Äô√† ~1500 √©valuations gratuites / jour) gr√¢ce √† l‚ÄôAPI **Google Gemini**.
  - **Sc√©nario d‚Äôusage recommand√©**:
    1. Ouvrir le notebook `RAG_Evaluation_Gemini.ipynb` dans Google Colab ou localement.  
    2. Configurer votre cl√© d‚ÄôAPI Gemini (variable d‚Äôenvironnement ou cellule d√©di√©e dans le notebook).  
    3. Charger un jeu de donn√©es depuis le dossier `REFERENTIEL`.  
    4. Appeler les fonctions de `rag_evaluator.py` pour:
       - Ex√©cuter les prompts/questions
       - Comparer les r√©ponses du mod√®le √† la v√©rit√© terrain
       - Calculer et exporter des m√©triques agr√©g√©es.
  - **Objectif**: compl√©ter l‚Äô√©valuation effectu√©e dans n8n par une approche code (Python) pour:
    - Lancer des campagnes d‚Äô√©valuation volumineuses
    - Automatiser des rapports qualit√©
    - Tester rapidement plusieurs variantes de prompts / workflows.

- **`EVALUATION/EVALUATION_Results` ‚Äì R√©sultats d‚Äô√©valuation**
  - `1-QA_Evaluations_output.csv`  
  - `2-QA_LuminaCorp_output.csv`  
  - `3-QA_Evaluations_pirate_output.csv`  
  - `4-QA_LuminaCorp_python_eval.csv`  
  - **R√¥le**: exemples de sorties g√©n√©r√©es par les workflows n8n et par le script Python, incluant :
    - Scores d‚Äô√©valuation
    - Comparaison entre r√©ponses attendues et r√©ponses LLM
    - D√©tection de d√©rives (ex. style ¬´ pirate ¬ª)

---

## Comment reproduire la d√©mo du meetup

- **1. Explorer la pr√©sentation**
  - Ouvrir `Evaluation workflow n8n.pdf` pour une vue d‚Äôensemble des concepts et du sc√©nario d√©mo.

- **2. Importer les workflows dans n8n**
  - Importer les fichiers du dossier `EVALUATION/JSON_QA_Evalutions` dans votre instance n8n.
  - Configurer vos credentials (LLM, Google Sheets, etc.).
  - Lancer les workflows pour:
    - G√©n√©rer des r√©ponses avec le LLM
    - Calculer des √©valuations QA
    - Exporter les r√©sultats (par ex. vers Google Sheets).

- **3. Lancer une √©valuation √† l‚Äô√©chelle avec Python & Gemini**
  - Ouvrir `RAG_Evaluation_Gemini.ipynb` (de pr√©f√©rence dans Google Colab).  
  - Renseigner votre cl√© API Gemini.  
  - Utiliser les CSV du dossier `REFERENTIEL` comme donn√©es d‚Äôentr√©e.  
  - Lancer les cellules qui appellent `rag_evaluator.py` pour effectuer une campagne d‚Äô√©valuation plus large.

---

## Objectif p√©dagogique

Ce d√©p√¥t est con√ßu pour vous aider √† :

- **Comprendre** les enjeux de l‚Äô√©valuation des LLM et des workflows n8n.  
- **Mettre en pratique** via des workflows n8n concrets (chargement de r√©f√©rentiels, g√©n√©ration, scoring, visualisation).  
- **Passer √† l‚Äô√©chelle** gr√¢ce √† un script Python et √† l‚ÄôAPI Gemini pour industrialiser vos √©valuations.  

N‚Äôh√©sitez pas √† cloner le d√©p√¥t, adapter les jeux de donn√©es √† vos propres cas d‚Äôusage et modifier les workflows / scripts pour vos besoins en production.


